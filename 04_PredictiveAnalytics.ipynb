{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Delay Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import required modules for prediction tasks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predicting whether a flight is late"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Data acquisition and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reads all predefined months for a year and merge into one data frame\n",
    "rawData2014 = pd.DataFrame.from_csv('cache/predictionData/complete2014Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print rawData2014.columns\n",
    "rawData2014.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cleaning the data set, we have to remove the following entries:\n",
    "\n",
    "- flights that have been cancelled or diverted. We focus on predicting the delay. As a result, we also remove the columns associated with diverted flights.\n",
    "- colmuns that give the answer. This is the case of many colmuns related to the arrival of the plane\n",
    "- rows where a value is missing\n",
    "\n",
    "Note that data points have to be cleaned in this order because most flights have empty entries for the 'diverted' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#entries to be dropped in the analysis\n",
    "columns_dropped = ['index', 'TAIL_NUM', 'FL_NUM', 'DEP_TIME', 'DEP_DELAY', 'TAXI_OUT', 'WHEELS_OFF', \\\n",
    "                   'WHEELS_ON', 'TAXI_IN', 'ARR_TIME', 'CANCELLED', 'CANCELLATION_CODE', 'AIR_TIME', \\\n",
    "                   'CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(data, list_col):\n",
    "    ''' \n",
    "    Creates a dataset by excluding undesirable columns\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    data: pandas.DataFrame\n",
    "       Flight dataframe  \n",
    "\n",
    "    list_col: <list 'string'>\n",
    "        Comumns to exclude from the data set\n",
    "    '''\n",
    "    \n",
    "    data.drop(data[data.CANCELLED == 0].index, inplace=True)\n",
    "    data.drop(list_col, axis=1, inplace=True)\n",
    "    data.dropna(axis = 0, inplace = True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data2014 = clean(rawData2014.copy(), columns_dropped)\n",
    "print data2014.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# save the data to avoid computing them again\n",
    "file_path = \"cache/predictionData/predictionData2014.csv\"\n",
    "data2014.to_csv(path_or_buf= file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# recover data2014 from cache/predictionData folder\n",
    "file_path = \"cache/predictionData/predictionData2014.csv\"\n",
    "data2014 = pd.read_csv(file_path)\n",
    "data2014.drop('Unnamed: 0', axis= 1, inplace = True)\n",
    "data2014.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test that clean did the job\n",
    "print \"size of raw data set: \", len(rawData2014)\n",
    "print \"number of cancelled: \", len(rawData2014[(rawData2014.CANCELLED == 1)])\n",
    "print \"size of data set: \", len(data2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an example that we will follow all along. The example looks a ta flight from New York to Chicago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>AIRCRAFT_YEAR</th>\n",
       "      <th>AIRCRAFT_MFR</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LONG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ORD</td>\n",
       "      <td>700</td>\n",
       "      <td>850</td>\n",
       "      <td>-17</td>\n",
       "      <td>733</td>\n",
       "      <td>1991</td>\n",
       "      <td>MCDONNELL DOUGLAS</td>\n",
       "      <td>40.766667</td>\n",
       "      <td>-73.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>AA</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ORD</td>\n",
       "      <td>700</td>\n",
       "      <td>850</td>\n",
       "      <td>32</td>\n",
       "      <td>733</td>\n",
       "      <td>2007</td>\n",
       "      <td>FRIEDEMANN JON</td>\n",
       "      <td>40.766667</td>\n",
       "      <td>-73.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>2014-01-05</td>\n",
       "      <td>AA</td>\n",
       "      <td>LGA</td>\n",
       "      <td>ORD</td>\n",
       "      <td>700</td>\n",
       "      <td>850</td>\n",
       "      <td>11</td>\n",
       "      <td>733</td>\n",
       "      <td>2007</td>\n",
       "      <td>FRIEDEMANN JON</td>\n",
       "      <td>40.766667</td>\n",
       "      <td>-73.866667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         FL_DATE UNIQUE_CARRIER ORIGIN DEST  CRS_DEP_TIME  CRS_ARR_TIME  \\\n",
       "2033  2014-01-01             AA    LGA  ORD           700           850   \n",
       "2034  2014-01-04             AA    LGA  ORD           700           850   \n",
       "2035  2014-01-05             AA    LGA  ORD           700           850   \n",
       "\n",
       "      ARR_DELAY  DISTANCE AIRCRAFT_YEAR                    AIRCRAFT_MFR  \\\n",
       "2033        -17       733          1991  MCDONNELL DOUGLAS                \n",
       "2034         32       733          2007  FRIEDEMANN JON                   \n",
       "2035         11       733          2007  FRIEDEMANN JON                   \n",
       "\n",
       "            LAT       LONG  \n",
       "2033  40.766667 -73.866667  \n",
       "2034  40.766667 -73.866667  \n",
       "2035  40.766667 -73.866667  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dexample = data2014[data2014.DEST == 'ORD'][(data2014.ORIGIN == 'JFK')  | (data2014.ORIGIN == 'LGA') | \\\n",
    "                                              (data2014.ORIGIN == 'EWR')].copy()\n",
    "\n",
    "dexample.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restricting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has more than 4 millions entries, which makes any data manipulation extremely costly - let alone model fitting. We will therefore make some restrictions on the airports and the airlines considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNIQUE_CARRIER\n",
       "AA    137960\n",
       "AS    157312\n",
       "B6    225303\n",
       "DL    644768\n",
       "EV    616920\n",
       "F9     81758\n",
       "FL     69486\n",
       "HA     73370\n",
       "MQ      2995\n",
       "OO    586067\n",
       "UA    471093\n",
       "US    381688\n",
       "VX     57056\n",
       "WN    597821\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2014.groupby('UNIQUE_CARRIER').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in the carrier that operates from New York to Chicago. Looking at the table, we also notice that Atlantic Southeast Airlines (airline code EV) is only marginally present. So we drop it from the list of carriers we will study in addition to the other carriers that do not operate on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNIQUE_CARRIER\n",
       "AA     126\n",
       "B6     955\n",
       "EV       2\n",
       "OO     191\n",
       "UA    6821\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dexample.groupby('UNIQUE_CARRIER').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def restrict_carrier(data, droplist):\n",
    "    ''' \n",
    "    Drop carriers from the data set.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    data: pandas.DataFrame\n",
    "       dataframe  \n",
    "\n",
    "    droplist: <list 'string'>\n",
    "        List of carriers to be droppped\n",
    "    '''        \n",
    "    \n",
    "    for item in droplist:\n",
    "        data.drop(data[data.UNIQUE_CARRIER == item].index, inplace= True)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of points 1420423\n",
      "airlines set(['AA', 'OO', 'B6', 'UA'])\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "drop_airline = [ 'AS','DL', 'EV', 'F9', 'FL', 'HA', 'MQ', 'US', 'VX', 'WN']\n",
    "restrict_carrier(data2014, drop_airline)\n",
    "print \"number of points\", len(data2014)\n",
    "print \"airlines\", set(data2014.UNIQUE_CARRIER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now focus on the main airports. We look for airports that have on average 50 domestic flight everyday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restrict_example(data):\n",
    "    ''' \n",
    "    Restrict dataset to airports for the example\n",
    "    '''  \n",
    "    \n",
    "    data.drop(data[(data.ORIGIN != 'JFK') & (data.ORIGIN != 'LGA') & (data.ORIGIN != 'EWR') & \\\n",
    "                  (data.DEST != 'ORD')].index, inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of points 205309\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "restrict_example(data2014)\n",
    "print \"number of points\", len(data2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove all airports that have an annual traffic under threshold\n",
    "\n",
    "def restrict_airport(data, threshold):\n",
    "    ''' \n",
    "    Drop carriers from the data set.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    data: pandas.DataFrame\n",
    "       dataframe  \n",
    "\n",
    "    droplist: <list 'string'>\n",
    "        List of carriers to be droppped\n",
    "    '''     \n",
    "    \n",
    "    dict_count = data.groupby(\"DEST\").agg(['count']).LAT.to_dict()['count']\n",
    "    \n",
    "    for key in dict_count:\n",
    "        if dict_count[key] < threshold:\n",
    "            data.drop(data[data.DEST == key].index, inplace=True)\n",
    "            data.drop(data[data.ORIGIN == key].index, inplace=True)\n",
    "    \n",
    "    print data.groupby(\"DEST\").agg(['count']).LAT.to_dict()['count']\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** WARNING ** \\\n",
    "RUN THIS CELL ONLY ONCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of airports:  12\n",
      "dataset size:  205309\n",
      "Wall time: 19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#restrict_airport(data2014, 60*365)\n",
    "data_example = data2014.copy()\n",
    "print \"number of airports: \", len(set(data2014))\n",
    "print \"dataset size: \", len(data2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save the restricted data to avoid computing them again\n",
    "file_path = \"cache/predictionData/data_example.csv\"\n",
    "data_example.to_csv(path_or_buf= file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>AIRCRAFT_YEAR</th>\n",
       "      <th>AIRCRAFT_MFR</th>\n",
       "      <th>LAT</th>\n",
       "      <th>LONG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>AA</td>\n",
       "      <td>JFK</td>\n",
       "      <td>LAX</td>\n",
       "      <td>900</td>\n",
       "      <td>1225</td>\n",
       "      <td>13</td>\n",
       "      <td>2475</td>\n",
       "      <td>1987</td>\n",
       "      <td>BOEING</td>\n",
       "      <td>40.633333</td>\n",
       "      <td>-73.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-01-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>JFK</td>\n",
       "      <td>LAX</td>\n",
       "      <td>900</td>\n",
       "      <td>1225</td>\n",
       "      <td>1</td>\n",
       "      <td>2475</td>\n",
       "      <td>1987</td>\n",
       "      <td>BOEING</td>\n",
       "      <td>40.633333</td>\n",
       "      <td>-73.783333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-01-04</td>\n",
       "      <td>AA</td>\n",
       "      <td>JFK</td>\n",
       "      <td>LAX</td>\n",
       "      <td>900</td>\n",
       "      <td>1225</td>\n",
       "      <td>59</td>\n",
       "      <td>2475</td>\n",
       "      <td>1986</td>\n",
       "      <td>BOEING</td>\n",
       "      <td>40.633333</td>\n",
       "      <td>-73.783333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      FL_DATE UNIQUE_CARRIER ORIGIN DEST  CRS_DEP_TIME  CRS_ARR_TIME  \\\n",
       "0  2014-01-01             AA    JFK  LAX           900          1225   \n",
       "1  2014-01-02             AA    JFK  LAX           900          1225   \n",
       "2  2014-01-04             AA    JFK  LAX           900          1225   \n",
       "\n",
       "   ARR_DELAY  DISTANCE AIRCRAFT_YEAR                    AIRCRAFT_MFR  \\\n",
       "0         13      2475          1987  BOEING                           \n",
       "1          1      2475          1987  BOEING                           \n",
       "2         59      2475          1986  BOEING                           \n",
       "\n",
       "         LAT       LONG  \n",
       "0  40.633333 -73.783333  \n",
       "1  40.633333 -73.783333  \n",
       "2  40.633333 -73.783333  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_example.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "# recover file\n",
    "#file_path = \"cache/predictionData/dataRes.csv\"\n",
    "#dataRes = pd.read_csv(file_path)\n",
    "#dataRes.drop('Unnamed: 0', axis= 1, inplace = True)\n",
    "#print dataRes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import strptime\n",
    "days = {0:\"Mon\", 1:\"Tues\", 2:\"Wed\", 3:\"Thurs\", 4:\"Fri\", 5:\"Sat\", 6:\"Sun\"}\n",
    "months = {1:\"Jan\", 2:\"Feb\", 3:\"Mar\", 4:\"Apr\", 5:\"May\", 6:\"June\", 7:\"July\", 8:\"Aug\", 9:\"Sep\", \\\n",
    "          10:\"Oct\", 11:\"Nov\", 12:\"Dec\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adjust_time(data):\n",
    "    monlist = np.empty(len(data), dtype = str)\n",
    "    daylist = np.empty(len(data), dtype = str)\n",
    "    \n",
    "    for i in xrange(len(data)):\n",
    "        date= strptime(data.FL_DATE.iloc[i], \"%Y-%M-%d\")\n",
    "        monlist[i] = months[date.tm_min]\n",
    "        daylist[i] = days[date.tm_wday]\n",
    "\n",
    "    return monlist, daylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'FL_DATE', u'UNIQUE_CARRIER', u'ORIGIN', u'DEST', u'CRS_DEP_TIME',\n",
       "       u'CRS_ARR_TIME', u'ARR_DELAY', u'DISTANCE', u'AIRCRAFT_YEAR',\n",
       "       u'AIRCRAFT_MFR', u'LAT', u'LONG'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_example.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Index([u'UNIQUE_CARRIER', u'ORIGIN', u'DEST', u'CRS_DEP_TIME', u'CRS_ARR_TIME',\n",
      "       u'ARR_DELAY', u'DISTANCE', u'AIRCRAFT_YEAR', u'AIRCRAFT_MFR', u'LAT',\n",
      "       u'LONG', u'MONTH', u'DAY'],\n",
      "      dtype='object')\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "monlist, daylist = adjust_time(data_example)\n",
    "print \"OK\"\n",
    "data_example['MONTH'] = pd.Series(monlist, index=data_example.index)\n",
    "data_example['DAY'] = pd.Series(daylist, index=data_example.index)\n",
    "if 'FL_DATE' in data_example.columns:\n",
    "    data_example.drop('FL_DATE', axis = 1, inplace= True)\n",
    "print data_example.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "Index([u'UNIQUE_CARRIER', u'ORIGIN', u'DEST', u'CRS_DEP_TIME', u'CRS_ARR_TIME',\n",
      "       u'ARR_DELAY', u'DISTANCE', u'AIRCRAFT_YEAR', u'AIRCRAFT_MFR', u'LAT',\n",
      "       u'LONG', u'MONTH', u'DAY'],\n",
      "      dtype='object')\n",
      "Wall time: 445 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "monlist, daylist = adjust_time(dexample)\n",
    "print \"OK\"\n",
    "dexample['MONTH'] = pd.Series(monlist, index=dexample.index)\n",
    "dexample['DAY'] = pd.Series(daylist, index=dexample.index)\n",
    "if 'FL_DATE' in dexample.columns:\n",
    "    dexample.drop('FL_DATE', axis = 1, inplace= True)\n",
    "print dexample.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the script to put time in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before change:  0    1225\n",
      "1    1225\n",
      "Name: CRS_ARR_TIME, dtype: int64\n",
      "\n",
      "after change:  0    745\n",
      "1    745\n",
      "Name: CRS_ARR_TIME_COR, dtype: int64\n",
      "Wall time: 825 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ti = lambda x: x/100*60+x%100\n",
    "data_example['CRS_ARR_TIME_COR'] = data_example.CRS_ARR_TIME.map(ti)\n",
    "data_example['CRS_DEP_TIME_COR'] = data_example.CRS_DEP_TIME.map(ti)\n",
    "data_example.drop(['CRS_DEP_TIME', 'CRS_ARR_TIME'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 92 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dexample['CRS_ARR_TIME_COR'] = dexample.CRS_ARR_TIME.map(ti)\n",
    "dexample['CRS_DEP_TIME_COR'] = dexample.CRS_DEP_TIME.map(ti)\n",
    "dexample.drop(['CRS_DEP_TIME', 'CRS_ARR_TIME'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to center and normalize all continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # change the age of the aircraft from a string type to an integer type\n",
    "data_example.drop(data_example[data_example.AIRCRAFT_YEAR =='    '].index, inplace = True)\n",
    "data_example['AIRCRAFT_YEAR_COR'] = data_example.AIRCRAFT_YEAR.map(lambda x: int(x))\n",
    "data_example.drop('AIRCRAFT_YEAR', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dexample.drop(dexample[dexample.AIRCRAFT_YEAR =='    '].index, inplace = True)\n",
    "dexample['AIRCRAFT_YEAR_COR'] = dexample.AIRCRAFT_YEAR.map(lambda x: int(x))\n",
    "dexample.drop('AIRCRAFT_YEAR', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(array):\n",
    "    mean = np.mean(array)\n",
    "    std = np.std(array)\n",
    "    return [(x - mean)/std for x in array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_data(data, feature_list):\n",
    "    ''' \n",
    "    Normalize data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    data: pandas.DataFrame\n",
    "       dataframe  \n",
    "\n",
    "    feature_list: <list 'string'>\n",
    "        List of features to be normalized\n",
    "    '''           \n",
    "\n",
    "    for feature in feature_list:\n",
    "        if feature in data.columns:\n",
    "            data[feature + '_NOR'] = normalize(data[feature].values)\n",
    "            data.drop(feature, axis =1, inplace=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "normalize_feature = ['CRS_DEP_TIME_COR', 'CRS_ARR_TIME_COR', 'DISTANCE', 'LONG', 'LAT', 'AIRCRAFT_YEAR_COR']\n",
    "normalize_data(data_example, normalize_feature)\n",
    "normalize_data(dexample, normalize_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are only interested in whetehr a flight will be more than 15 minutes late. So we adjust the ARR_DELAY colum to an indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_example['ARR_DELAY_COR'] = data_example.ARR_DELAY.map(lambda x: (x >= 15))\n",
    "#data_example.drop('ARR_DELAY', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_list = ['UNIQUE_CARRIER', 'ORIGIN', 'DEST', 'AIRCRAFT_MFR', 'MONTH','DAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "finalData_example = pd.get_dummies(data_example, columns=encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# save the restricted data to avoid computing them again\n",
    "file_path = \"cache/predictionData/finalData_example.csv\"\n",
    "finalData_example.to_csv(path_or_buf= file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "# recover data2014 from cache/predictionData folder\n",
    "#file_path = \"cache/predictionData/finalData.csv\"\n",
    "#finalData = pd.read_csv(file_path)\n",
    "#finalData.drop('Unnamed: 0', axis= 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Baseline classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make prediction on the variable 'ARR_DEL15'. This variable takes the value 1 is the plane is more than 15 minutes late and 0 if not. Let's look at the baseline classifier, that is the classifiers that assign repectively 1 or 0 to 'ARR_DEL15' for every flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "def baseline(data, target):\n",
    "    ''' \n",
    "    Compute the baseline classifiers along a target variable for a data set data\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    data: pandas.DataFrame\n",
    "       dataframe  \n",
    "\n",
    "    target: string\n",
    "        Column of data along wich we compute the baseline classifiers\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    score_baseline_1 = np.size(data[data[target] == 1][target].values) / np.size(data[target].values)\n",
    "    score_baseline_0 = np.size(data[data[target] == 0][target].values) / np.size(data[target].values)\n",
    "    \n",
    "    print \"baseline classifier everyone to 0: \", int(score_baseline_0*100) , \"%\"\n",
    "    print \"baseline classifier everyone to 1: \", int(score_baseline_1*100) , \"%\"\n",
    "   \n",
    "    return score_baseline_0, score_baseline_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#score_baseline_0, score_baseline_1 = baseline(finalData_example, 'ARR_DELAY_COR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's split the data set into a training set and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split(data, list_drop, target, test_size):\n",
    "    ''' \n",
    "    Splits the data into a training and a test set\n",
    "    Separates the training and test sets according to a feature set and a target set\n",
    "    Balance the features sets by retaining only fraction of its points\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    data: pandas.DataFrame\n",
    "       Flight dataframe  \n",
    "\n",
    "    list_drop: <list 'string'>\n",
    "        List of columns to exclude from the features set\n",
    "        \n",
    "    target: string\n",
    "        target column along whch we make the target set\n",
    "        \n",
    "    test_size: float\n",
    "        size of the test set\n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    #split the dataset into a training set and a test set\n",
    "    dtrain, dtest = train_test_split(data, test_size = 0.3)\n",
    "    \n",
    "    Xtrain = dtrain.drop(list_drop, axis=1).values\n",
    "    ytrain = dtrain[target].values\n",
    "    Xtest = dtest.drop(list_drop, axis=1).values\n",
    "    ytest = dtest[target].values\n",
    "    \n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ARR_DELAY', u'CRS_DEP_TIME_COR_NOR', u'CRS_ARR_TIME_COR_NOR',\n",
       "       u'DISTANCE_NOR', u'LONG_NOR', u'LAT_NOR', u'AIRCRAFT_YEAR_COR_NOR',\n",
       "       u'UNIQUE_CARRIER_AA', u'UNIQUE_CARRIER_B6', u'UNIQUE_CARRIER_OO', \n",
       "       ...\n",
       "       u'MONTH_J', u'MONTH_M', u'MONTH_N', u'MONTH_O', u'MONTH_S', u'DAY_F',\n",
       "       u'DAY_M', u'DAY_S', u'DAY_T', u'DAY_W'],\n",
       "      dtype='object', length=227)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalData_example.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain, ytrain, Xtest, ytest = split(finalData_example, ['ARR_DELAY'], 'ARR_DELAY', 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_random_forest(Xtrain, ytrain, Xtest, ytest, n_trees=10, max_features='auto'):\n",
    "    ''' \n",
    "    Fits a random forest with (Xtrain ,ytrain)\n",
    "    Computes the score on (Xtest, ytest)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    Xtrain: numpy 2D array\n",
    "       Feature training set\n",
    "\n",
    "    ytrain: numpy 1D array\n",
    "        Target training set\n",
    "    \n",
    "    Xtest: numpy 2D array\n",
    "       Feature test set\n",
    "\n",
    "    ytest: numpy 1D array\n",
    "        Target test set\n",
    "    \n",
    "    n_trees: int\n",
    "        number of trees in the forest\n",
    "    \n",
    "    max_features: string or int\n",
    "        number of features used for every tree\n",
    "        \n",
    "    Outputs:\n",
    "    --------\n",
    "    \n",
    "    score_train: float\n",
    "        score on the train set\n",
    "    \n",
    "    score_test: float\n",
    "        score on the test set\n",
    "    \n",
    "    clf.feature_importances_\n",
    "        weights of each feature as used by the classifier\n",
    "    \n",
    "    ''' \n",
    "\n",
    "    clf= RandomForestRegressor(n_estimators=n_trees, max_features= max_features)\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    \n",
    "    score_train = mean_squared_error(clf.predict(Xtrain), ytrain)\n",
    "    score_test = mean_squared_error(clf.predict(Xtest), ytest)\n",
    "    \n",
    "    return  score_train, score_test, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_parameters(Xtrain, ytrain, Xtest, ytest, nb_trees, nb_features):\n",
    "    ''' \n",
    "    Fits sequentially random forest classifiers\n",
    "    Adds each test score in a pandas.DataFrame with the number of trees, the loss function, the train score,\n",
    "    and the importance of each features\n",
    "    Returns a DataFrame with all scores\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    Xtrain: numpy 2D array\n",
    "       Feature training set\n",
    "\n",
    "    ytrain: numpy 1D array\n",
    "        Target training set\n",
    "    \n",
    "    Xtest: numpy 2D array\n",
    "       Feature test set\n",
    "\n",
    "    ytest: numpy 1D array\n",
    "        Target test set\n",
    "    \n",
    "    n_trees: <list int>\n",
    "        list of numbers of trees in the forest\n",
    "    \n",
    "    nb_features: <list int>\n",
    "        list of number of features in the forest\n",
    "        \n",
    "    Outputs:\n",
    "    --------\n",
    "    \n",
    "    score_tab: pandas.DataFrame\n",
    "        DataFrame of scores with associated parameters\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    score_tab = pd.DataFrame(columns=['nb_trees', 'nb_features', 'test_score', 'train_score', 'classifier'])\n",
    "    \n",
    "    # counter will increment the index in score_tab\n",
    "    counter = 0 \n",
    "\n",
    "    for n_estimators in nb_trees:\n",
    "        for max_features in nb_features:\n",
    "\n",
    "            score_train, score_test, classifier = \\\n",
    "            score_random_forest(Xtrain, ytrain, Xtest, ytest, n_trees=n_estimators, max_features=max_features) \n",
    "            score_tab.loc[counter] = [n_estimators, max_features, score_test, score_train, classifier]\n",
    "            counter += 1\n",
    "\n",
    "    return score_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_random_forest(data, list_drop, target, test_size=0.4, nb_trees=[10], nb_features = ['auto']):\n",
    "    Xtrain, ytrain, Xtest, ytest = split(data, list_drop, target, test_size)\n",
    "    scores =  best_parameters(Xtrain, ytrain, Xtest, ytest, nb_trees, nb_features)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_trees = [50]\n",
    "nb_features = ['log2']\n",
    "test_size = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nb_trees nb_features   test_score  train_score  \\\n",
      "0        50        log2  2344.664596   549.668531   \n",
      "\n",
      "                                          classifier  \n",
      "0  (DecisionTreeRegressor(criterion='mse', max_de...  \n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "randomForest2014 =  classify_random_forest(finalData_example, ['ARR_DELAY'], 'ARR_DELAY', test_size=test_size, nb_trees=nb_trees, nb_features=nb_features)\n",
    "print randomForest2014.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save file to /data/ folder\n",
    "file_path = \"cache/predictionData/randomForest2014.csv\"\n",
    "randomForest2014.to_csv(path_or_buf= file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the importance coefficients, that the average usage of each coefficients in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_clf = randomForest2014[randomForest2014.test_score = np.max(randomForest2014.test_score.values)].classifier.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xexample = dexample.drop(list_drop, axis=1).values\n",
    "yexample = dexample[target].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Describe Process*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Predicting flight delay time with Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow users to enjoy more than a classfication experience we want to give them an expected delay time in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import required modules for prediction tasks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import zipfile\n",
    "import StringIO\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn functions used for the linear regression model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us again establish a baseline which has to be beaten by our model. To get a feeling for a good baseline we pick flights from from New York(all airports) to Chicago(all airports)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "# first step is to load the actual data and exclude rows that are unnecessary\n",
    "# a script that produces the csv file can be found in the src folder\n",
    "# (it might take some while to run, on my macbook up to one hour)\n",
    "print('loading data...')\n",
    "bigdf = pd.read_csv('cache/Big5FlightTable.csv')\n",
    "\n",
    "years = ['2010', '2011', '2012', '2013', '2014']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# specify here which cities should be investigated\n",
    "city_from = 'New York, NY'\n",
    "city_to = 'Chicago, IL'\n",
    "\n",
    "# filter for cities\n",
    "bigdf = bigdf[(bigdf.ORIGIN_CITY_NAME == city_from) & (bigdf.DEST_CITY_NAME == city_to)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. A first predictor\n",
    "For a span of days we are interested in knoweledge, which flight we should take. Therefore let's choose the popular date for Christmas returning flights 21.12.2015 as example. As we want to use historical data, let's get first clear what data we have. Is it possible to compare flights over the years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x234cb07d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAC5CAYAAAA4eHs5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMAAYQkAYICwBAAwQFgCABjwfytdxrsp1CfUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1123a82d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_day = 21\n",
    "query_month = 12\n",
    "\n",
    "# how many flights do exist in all years?\n",
    "flights = []\n",
    "flightvalues = []\n",
    "for y in years:\n",
    "    query = list(bigdf[(bigdf.YEAR == int(y)) & (bigdf.MONTH == query_month) & (bigdf.DAY_OF_MONTH == query_day)].FL_NUM.astype(int).unique())\n",
    "    flights.append(query)\n",
    "    flightvalues += query\n",
    "    \n",
    "# build a matrix\n",
    "data_matrix = np.zeros((len(flightvalues), len(years)))\n",
    "# build dict\n",
    "flightdict = dict(zip(flightvalues, np.arange(0, len(flightvalues))))\n",
    "\n",
    "# fill datamatrix\n",
    "for i in xrange(len(years)):\n",
    "    for j in flights[i]:\n",
    "        data_matrix[flightdict[j], i] = 1.\n",
    "        \n",
    "# plot matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(data_matrix, extent=[0,data_matrix.shape[0],0,data_matrix.shape[1] * 5], \\\n",
    "           interpolation='none', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, flight numbers are not stable over the years and it is not trivial to find a matching.\n",
    "Thus comparison by features for individual flights is not really possible. \n",
    "It seems as if airlines change their flight numbers on a yearly basis. \n",
    "Thus we need to come up with another idea and use a latent variable based approach instead.\n",
    "One of the easiest ideas is to average the delay time over each day as a base classifier and refine then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dffordate = bigdf[bigdf.MONTH == query_month]\n",
    "dffordate = dffordate[dffordate.DAY_OF_MONTH == query_day]\n",
    "dffordate.head()\n",
    "\n",
    "def predict_base_model(X):\n",
    "    return np.array([dffordate.ARR_DELAY.mean()]*X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the quality of this model, we use the last year as test set and the previous as train data. The idea is, that we are always interested in predicting the next year somehow. Thus, if the match for 2014 is good, we expect it to be the same for 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build test/train set\n",
    "df_train = dffordate[dffordate.YEAR != int(years[-1])]\n",
    "df_test = dffordate[dffordate.YEAR == int(years[-1])]\n",
    "\n",
    "y_train = df_train.ARR_DELAY\n",
    "X_train = y_train # here dummy\n",
    "y_test = df_test.ARR_DELAY\n",
    "X_test = y_test # here dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the base model, the prediction for 2014 is that we are going to be 27.72 minutes late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.728260869565219"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict_base_model(X_test)\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good did it perform comparing the actual arrival delay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.10756003440288"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(((y - y_pred)**2).mean())\n",
    "\n",
    "rmse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the root mean squared error as one (of many possible) measures, any model that we develop should beat the benchmark of 44."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Building a linear regression model for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As flight delay changes over the time of day like explored in the data exploration part, we introduce a new feature which models in a categorical variable 10 minute time windows. I.e. for each window we introduce a latent variable that captures some sort of delay influence of this frame. This is done for both the departure and the arrival time. The model here is first developed for the reduced dataset containing the flights between New York / Chicago only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.05 s, sys: 212 ms, total: 3.27 s\n",
      "Wall time: 3.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigdf['HOUR_OF_ARR'] = 0\n",
    "bigdf['HOUR_OF_DEP'] = 0\n",
    "\n",
    "for index, row in bigdf.iterrows():\n",
    "    bigdf.set_value(index, 'HOUR_OF_ARR', int(row['ARR_TIME']) / 10)\n",
    "    bigdf.set_value(index, 'HOUR_OF_DEP', int(row['DEP_TIME']) / 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, before fitting the actual model categorical variables need to be encoded as binary features (they have no order!) and numerical features shall be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into numerical and categorical features\n",
    "numericalFeat = bigdf[['DISTANCE', 'AIRCRAFT_AGE']].astype('float') \n",
    "categoricalFeat = bigdf[['MONTH', 'DAY_OF_MONTH', 'ORIGIN', 'DEST', \\\n",
    "                         'HOUR_OF_ARR', 'HOUR_OF_DEP', 'UNIQUE_CARRIER', \\\n",
    "                         'DAY_OF_WEEK', 'AIRCRAFT_MFR']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with categorical variables\n",
    "Luckily, sklearn has a routine to do this for us. Yet, as it is only able to handle integers we first reindex all categorical features we create lookup tables for the values. This turned out to be one of the slowest step in processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/site-packages/pandas/core/indexing.py:415: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 104 ms, total: 13 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for the next step, all features need to be encoded as integers --> create lookup Tables!\n",
    "def transformToID(df, col):\n",
    "    vals = df[col].unique()\n",
    "    LookupTable = dict(zip(vals, np.arange(len(vals))))\n",
    "    for key in LookupTable.keys():\n",
    "        df[df[col] == key] = LookupTable[key]\n",
    "    return (LookupTable, df)\n",
    "\n",
    "mfrDict, categoricalFeat = transformToID(categoricalFeat, 'AIRCRAFT_MFR')\n",
    "originDict, categoricalFeat = transformToID(categoricalFeat, 'ORIGIN')\n",
    "destDict, categoricalFeat = transformToID(categoricalFeat, 'DEST')\n",
    "carrierDict, categoricalFeat = transformToID(categoricalFeat, 'UNIQUE_CARRIER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>HOUR_OF_ARR</th>\n",
       "      <th>HOUR_OF_DEP</th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>AIRCRAFT_MFR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MONTH  DAY_OF_MONTH ORIGIN DEST  HOUR_OF_ARR  HOUR_OF_DEP UNIQUE_CARRIER  \\\n",
       "921      0             0      0    0            0            0              0   \n",
       "922      1             1      1    1            1            1              1   \n",
       "923      0             0      0    0            0            0              0   \n",
       "924      0             0      0    0            0            0              0   \n",
       "925      2             2      2    2            2            2              2   \n",
       "\n",
       "     DAY_OF_WEEK AIRCRAFT_MFR  \n",
       "921            0            0  \n",
       "922            1            1  \n",
       "923            0            0  \n",
       "924            0            0  \n",
       "925            2            2  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricalFeat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn the variables are now encoded. As we have many variables (around 1200 in the final model for the whole year) using sparse matrices was critical for us to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.2 ms, sys: 10 ms, total: 58.3 ms\n",
      "Wall time: 59.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "encoder = OneHotEncoder() \n",
    "categoricals_encoded = encoder.fit_transform(categoricalFeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recombining categorical and numerical features allows to start the usual Machine Learning routine (split into test/train, normalize, train/cross-validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/scipy/sparse/compressed.py:739: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "numericals_sparse = sparse.csr_matrix(numericalFeat)\n",
    "# get data matrix & response variable\n",
    "X_all = sparse.hstack((numericals_sparse, categoricals_encoded))\n",
    "y_all = bigdf['ARR_DELAY'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# we have 2 numerical features\n",
    "X_train_numericals = X_train[:, 0:3].toarray()\n",
    "X_test_numericals = X_test[:, 0:3].toarray()\n",
    "\n",
    "# use sklearn tools to standardize numerical features...\n",
    "scaler = StandardScaler() \n",
    "scaler.fit(X_train_numericals) # get std/mean from train set\n",
    "X_train_numericals = sparse.csr_matrix(scaler.transform(X_train_numericals)) \n",
    "X_test_numericals = sparse.csr_matrix(scaler.transform(X_test_numericals))\n",
    "\n",
    "# update sets\n",
    "X_train[:, 0:3] = X_train_numericals\n",
    "X_test[:, 0:3] = X_test_numericals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal least squares regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.323925698812779"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# fit the model!\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "rmse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first model, we use OLS to get a feeling of what is achievable using a reduced dataset. The motivation for this is mainly to see whether this might be used to speedup the whole process. As the RMSE reveals, we did not do better than the base classifier. What happened? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31881, 74), (74,))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, clf.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number of variables (74 here), it might be that either the fit is not good enough or we are ignoring the dependency of flight delays within different routes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.01 s, sys: 52 ms, total: 2.06 s\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use ridge regression (i.e. Gaussian prior) and vary the lambda parameter using Grid search\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "SGD_params = {'alpha': 10.0 ** -np.arange(-2,8)}\n",
    "SGD_model = GridSearchCV(SGDRegressor(random_state = 42), \\\n",
    "                         SGD_params, scoring = 'mean_absolute_error', cv = 6) # cross validate 6 times\n",
    "\n",
    "\n",
    "# train the model, this might take some time...\n",
    "SGD_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.389335267933092"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = SGD_model.predict(X_test)\n",
    "rmse(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the RMSE again is a bit disappointing. We did worse than OLS and the base classifier! This means, it is now time to tackle the unavoidable: Regressing the whole dataset!\n",
    "\n",
    "As the data becomes rapidly huge (for 1 year ~ 700MB uncompressed CSV, 5 years ~ 3.5GB, 10 years ~ 7.2GB) the code to perform the actual regression has been developed first in an IPython notebook and then run separately. It can be found in the `src` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression over the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
