{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import required modules for prediction tasks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import zipfile\n",
    "import StringIO\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# sklearn functions used for the linear regression model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "# first step is to load the actual data and exclude rows that are unnecessary\n",
    "print('loading data...')\n",
    "df = pd.read_csv('cache/BigFlightTable.csv', nrows=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns found: \n",
      "Index([u'Unnamed: 0', u'index', u'ORIGIN_CITY_NAME', u'ARR_DEL15', u'FL_NUM',\n",
      "       u'CANCELLED', u'ARR_DELAY', u'MONTH', u'DIVERTED', u'DAY_OF_MONTH',\n",
      "       u'DEST_CITY_NAME', u'ORIGIN', u'DEP_TIME', u'DEST', u'ARR_DELAY_NEW',\n",
      "       u'DAY_OF_WEEK', u'YEAR', u'AIRLINE_ID', u'QUARTER', u'DISTANCE',\n",
      "       u'ORIGIN_STATE_NM', u'ARR_TIME', u'UNIQUE_CARRIER', u'ORIGIN_WAC',\n",
      "       u'TAIL_NUM', u'AIRCRAFT_YEAR', u'AIRCRAFT_MFR', u'AIRCRAFT_AGE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print 'columns found: '\n",
    "print df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating additional features\n"
     ]
    }
   ],
   "source": [
    "print 'generating additional features'\n",
    "df['HOUR_OF_ARR'] = df['ARR_TIME'].astype(int) / 10\n",
    "df['HOUR_OF_DEP'] = df['DEP_TIME'].astype(int) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting into numerical/categorical features\n"
     ]
    }
   ],
   "source": [
    "# split data into numerical and categorical features\n",
    "print 'splitting into numerical/categorical features'\n",
    "numericalFeat = df[['DISTANCE', 'AIRCRAFT_AGE']].copy().astype('float') # Numerical features\n",
    "num_numFeatures = 2\n",
    "categoricalFeat = df[['MONTH', 'DAY_OF_MONTH', 'ORIGIN', \n",
    "                    'DEST', 'HOUR_OF_ARR', 'HOUR_OF_DEP', \n",
    "                    'UNIQUE_CARRIER', 'DAY_OF_WEEK', 'AIRCRAFT_MFR']].copy() # Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for the next step, all features need to be encoded as integers --> create lookup Tables!\n",
    "def transformToID(df, col):\n",
    "    vals = df[col].unique()\n",
    "    LookupTable = dict(zip(vals, np.arange(len(vals))))\n",
    "    for key in LookupTable.keys():\n",
    "        df.loc[df[col] == key, col] = LookupTable[key]\n",
    "    return LookupTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing UNIQUE_CARRIER\n",
      "indexing AIRCRAFT_MFR\n",
      "indexing DEST\n",
      "indexing ORIGIN\n"
     ]
    }
   ],
   "source": [
    "print 'indexing UNIQUE_CARRIER'\n",
    "carrierTable = transformToID(categoricalFeat, 'UNIQUE_CARRIER')\n",
    "with open('cache/carrierTable.json', 'wb') as outfile:\n",
    "    json.dump(carrierTable, outfile)\n",
    "print 'indexing AIRCRAFT_MFR'\n",
    "mfrTable = transformToID(categoricalFeat, 'AIRCRAFT_MFR')\n",
    "with open('cache/manufacturerTable.json', 'wb') as outfile:\n",
    "    json.dump(mfrTable, outfile)\n",
    "    \n",
    "\n",
    "print 'indexing DEST'\n",
    "destTable = transformToID(categoricalFeat, 'DEST')\n",
    "with open('cache/destTable.json', 'wb') as outfile:\n",
    "    json.dump(destTable, outfile)\n",
    "print 'indexing ORIGIN'\n",
    "originTable = transformToID(categoricalFeat, 'ORIGIN')\n",
    "with open('cache/originTable.json', 'wb') as outfile:\n",
    "    json.dump(originTable, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding categorical variables\n",
      "splitting test/train set\n",
      "normalizing numerical features\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables as binary ones\n",
    "print 'encoding categorical variables'\n",
    "encoder = OneHotEncoder() \n",
    "categoricals_encoded = encoder.fit_transform(categoricalFeat)\n",
    "\n",
    "# convert numerical features to sparse matrix\n",
    "numericals_sparse = sparse.csr_matrix(numericalFeat)\n",
    "\n",
    "# get data matrix & response variable\n",
    "X_all = sparse.hstack((numericals_sparse, categoricals_encoded))\n",
    "y_all = df['ARR_DELAY'].values\n",
    "\n",
    "# construct test/train set (15%)\n",
    "print 'splitting test/train set'\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.15, random_state = 42)\n",
    "\n",
    "# before starting the regression, numerical features need to be standardized!\n",
    "X_train_numericals = X_train[:, 0:num_numFeatures+1].toarray()\n",
    "X_test_numericals = X_test[:, 0:num_numFeatures+1].toarray()\n",
    "\n",
    "# use sklearn tools...\n",
    "print 'normalizing numerical features'\n",
    "scaler = StandardScaler() \n",
    "scaler.fit(X_train_numericals) # get std/mean from train set\n",
    "\n",
    "# save scaler to cache (for later prediction)\n",
    "with open('cache/scalerValues.csv', 'wb') as f:\n",
    "    f.write('mean: ' + str(list(scaler.mean_)) + '\\n')\n",
    "    f.write('std : ' + str(list(scaler.std_)) + '\\n')\n",
    "\n",
    "X_train_numericals = sparse.csr_matrix(scaler.transform(X_train_numericals)) \n",
    "X_test_numericals = sparse.csr_matrix(scaler.transform(X_test_numericals))\n",
    "\n",
    "# update sets\n",
    "X_train[:, 0:num_numFeatures+1] = X_train_numericals\n",
    "X_test[:, 0:num_numFeatures+1] = X_test_numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stochastic gradient based ridge regression\n",
    "SGD_params = {'alpha': 10.0 ** -np.arange(1,8)}\n",
    "SGD_model = GridSearchCV(SGDRegressor(random_state = 42, verbose=1), \\\n",
    "                         SGD_params, scoring = 'mean_absolute_error', cv = 4) # cross validate 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 12.38, NNZs: 368, Bias: 0.058037, T: 1275, Avg. loss: 1462.066434\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 18.14, NNZs: 368, Bias: 0.089444, T: 2550, Avg. loss: 1440.205982\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 20.58, NNZs: 368, Bias: 0.088980, T: 3825, Avg. loss: 1424.935557\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 23.51, NNZs: 368, Bias: 0.109310, T: 5100, Avg. loss: 1415.373491\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 25.41, NNZs: 368, Bias: 0.119647, T: 6375, Avg. loss: 1407.352229\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 15.89, NNZs: 364, Bias: 0.074917, T: 1275, Avg. loss: 1896.165625\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 21.09, NNZs: 364, Bias: 0.096697, T: 2550, Avg. loss: 1866.711929\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 24.78, NNZs: 364, Bias: 0.110052, T: 3825, Avg. loss: 1846.926397\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 27.75, NNZs: 364, Bias: 0.130703, T: 5100, Avg. loss: 1833.330544\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 29.79, NNZs: 364, Bias: 0.141341, T: 6375, Avg. loss: 1822.374494\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 15.78, NNZs: 366, Bias: 0.080481, T: 1275, Avg. loss: 1583.453817\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.31, NNZs: 366, Bias: 0.096592, T: 2550, Avg. loss: 1557.317569\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 24.40, NNZs: 366, Bias: 0.120670, T: 3825, Avg. loss: 1540.327113\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 26.73, NNZs: 366, Bias: 0.126153, T: 5100, Avg. loss: 1527.447301\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.66, NNZs: 366, Bias: 0.138867, T: 6375, Avg. loss: 1517.281216\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 15.45, NNZs: 360, Bias: 0.070336, T: 1275, Avg. loss: 1904.584570\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.16, NNZs: 360, Bias: 0.092003, T: 2550, Avg. loss: 1876.202798\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 23.88, NNZs: 360, Bias: 0.107218, T: 3825, Avg. loss: 1858.498428\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 26.06, NNZs: 360, Bias: 0.105463, T: 5100, Avg. loss: 1844.835697\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.34, NNZs: 360, Bias: 0.125373, T: 6375, Avg. loss: 1835.034932\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 14.08, NNZs: 368, Bias: 0.049031, T: 1275, Avg. loss: 1461.826315\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 21.74, NNZs: 368, Bias: 0.072237, T: 2550, Avg. loss: 1435.287004\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 26.66, NNZs: 368, Bias: 0.063439, T: 3825, Avg. loss: 1416.265217\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 31.78, NNZs: 368, Bias: 0.076175, T: 5100, Avg. loss: 1402.121665\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 36.03, NNZs: 368, Bias: 0.079746, T: 6375, Avg. loss: 1390.016281\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.52, NNZs: 364, Bias: 0.066086, T: 1275, Avg. loss: 1894.720702\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.17, NNZs: 364, Bias: 0.078727, T: 2550, Avg. loss: 1860.325038\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.46, NNZs: 364, Bias: 0.083782, T: 3825, Avg. loss: 1835.099482\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.96, NNZs: 364, Bias: 0.096056, T: 5100, Avg. loss: 1815.981562\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.63, NNZs: 364, Bias: 0.098909, T: 6375, Avg. loss: 1800.055780\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.32, NNZs: 366, Bias: 0.072430, T: 1275, Avg. loss: 1581.502724\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.30, NNZs: 366, Bias: 0.079819, T: 2550, Avg. loss: 1551.137487\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 30.80, NNZs: 366, Bias: 0.095931, T: 3825, Avg. loss: 1528.641862\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 35.79, NNZs: 366, Bias: 0.092689, T: 5100, Avg. loss: 1510.889519\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 40.38, NNZs: 366, Bias: 0.097631, T: 6375, Avg. loss: 1496.055861\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 16.87, NNZs: 360, Bias: 0.062876, T: 1275, Avg. loss: 1904.201711\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 23.98, NNZs: 360, Bias: 0.077626, T: 2550, Avg. loss: 1870.687807\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 30.24, NNZs: 360, Bias: 0.085531, T: 3825, Avg. loss: 1847.661128\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 35.22, NNZs: 360, Bias: 0.076019, T: 5100, Avg. loss: 1829.307735\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 40.11, NNZs: 360, Bias: 0.089079, T: 6375, Avg. loss: 1814.469552\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 14.27, NNZs: 368, Bias: 0.048095, T: 1275, Avg. loss: 1461.811409\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.17, NNZs: 368, Bias: 0.070426, T: 2550, Avg. loss: 1434.748882\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.41, NNZs: 368, Bias: 0.060746, T: 3825, Avg. loss: 1415.287819\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 32.85, NNZs: 368, Bias: 0.072656, T: 5100, Avg. loss: 1400.580694\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 37.47, NNZs: 368, Bias: 0.075497, T: 6375, Avg. loss: 1387.951237\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.70, NNZs: 364, Bias: 0.065168, T: 1275, Avg. loss: 1894.575807\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.65, NNZs: 364, Bias: 0.076838, T: 2550, Avg. loss: 1859.635997\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 32.28, NNZs: 364, Bias: 0.081008, T: 3825, Avg. loss: 1833.789291\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 38.14, NNZs: 364, Bias: 0.092363, T: 5100, Avg. loss: 1814.013063\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 43.20, NNZs: 364, Bias: 0.094367, T: 6375, Avg. loss: 1797.474285\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.49, NNZs: 366, Bias: 0.071592, T: 1275, Avg. loss: 1581.299171\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.76, NNZs: 366, Bias: 0.078059, T: 2550, Avg. loss: 1550.465815\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.58, NNZs: 366, Bias: 0.093312, T: 3825, Avg. loss: 1527.332720\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.94, NNZs: 366, Bias: 0.089118, T: 5100, Avg. loss: 1508.996356\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.93, NNZs: 366, Bias: 0.093205, T: 6375, Avg. loss: 1493.587418\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.04, NNZs: 360, Bias: 0.062099, T: 1275, Avg. loss: 1904.170057\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.43, NNZs: 360, Bias: 0.076122, T: 2550, Avg. loss: 1870.077116\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.02, NNZs: 360, Bias: 0.083248, T: 3825, Avg. loss: 1846.433945\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.39, NNZs: 360, Bias: 0.072905, T: 5100, Avg. loss: 1827.521759\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.67, NNZs: 360, Bias: 0.085212, T: 6375, Avg. loss: 1812.062195\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 14.29, NNZs: 368, Bias: 0.048001, T: 1275, Avg. loss: 1461.810020\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.21, NNZs: 368, Bias: 0.070244, T: 2550, Avg. loss: 1434.694582\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.49, NNZs: 368, Bias: 0.060475, T: 3825, Avg. loss: 1415.188898\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 32.96, NNZs: 368, Bias: 0.072302, T: 5100, Avg. loss: 1400.424242\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 37.61, NNZs: 368, Bias: 0.075069, T: 6375, Avg. loss: 1387.741067\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.72, NNZs: 364, Bias: 0.065075, T: 1275, Avg. loss: 1894.561320\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.70, NNZs: 364, Bias: 0.076648, T: 2550, Avg. loss: 1859.566575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 32.36, NNZs: 364, Bias: 0.080729, T: 3825, Avg. loss: 1833.656955\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 38.26, NNZs: 364, Bias: 0.091991, T: 5100, Avg. loss: 1813.813778\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 43.36, NNZs: 364, Bias: 0.093909, T: 6375, Avg. loss: 1797.212477\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.51, NNZs: 366, Bias: 0.071508, T: 1275, Avg. loss: 1581.278731\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.81, NNZs: 366, Bias: 0.077882, T: 2550, Avg. loss: 1550.398087\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.66, NNZs: 366, Bias: 0.093049, T: 3825, Avg. loss: 1527.200318\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 37.06, NNZs: 366, Bias: 0.088758, T: 5100, Avg. loss: 1508.804527\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 42.09, NNZs: 366, Bias: 0.092759, T: 6375, Avg. loss: 1493.336915\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.05, NNZs: 360, Bias: 0.062021, T: 1275, Avg. loss: 1904.166969\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.48, NNZs: 360, Bias: 0.075970, T: 2550, Avg. loss: 1870.015410\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.10, NNZs: 360, Bias: 0.083019, T: 3825, Avg. loss: 1846.309685\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.51, NNZs: 360, Bias: 0.072592, T: 5100, Avg. loss: 1827.340670\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.83, NNZs: 360, Bias: 0.084823, T: 6375, Avg. loss: 1811.817700\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 14.29, NNZs: 368, Bias: 0.047992, T: 1275, Avg. loss: 1461.809882\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.22, NNZs: 368, Bias: 0.070226, T: 2550, Avg. loss: 1434.689148\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.50, NNZs: 368, Bias: 0.060448, T: 3825, Avg. loss: 1415.178994\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 32.97, NNZs: 368, Bias: 0.072266, T: 5100, Avg. loss: 1400.408573\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 37.63, NNZs: 368, Bias: 0.075027, T: 6375, Avg. loss: 1387.720013\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.72, NNZs: 364, Bias: 0.065066, T: 1275, Avg. loss: 1894.559871\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.70, NNZs: 364, Bias: 0.076629, T: 2550, Avg. loss: 1859.559628\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 32.37, NNZs: 364, Bias: 0.080701, T: 3825, Avg. loss: 1833.643708\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 38.27, NNZs: 364, Bias: 0.091954, T: 5100, Avg. loss: 1813.793825\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 43.38, NNZs: 364, Bias: 0.093863, T: 6375, Avg. loss: 1797.186259\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.51, NNZs: 366, Bias: 0.071500, T: 1275, Avg. loss: 1581.276687\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.82, NNZs: 366, Bias: 0.077865, T: 2550, Avg. loss: 1550.391309\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.67, NNZs: 366, Bias: 0.093022, T: 3825, Avg. loss: 1527.187063\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 37.08, NNZs: 366, Bias: 0.088722, T: 5100, Avg. loss: 1508.785319\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 42.11, NNZs: 366, Bias: 0.092714, T: 6375, Avg. loss: 1493.311828\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.05, NNZs: 360, Bias: 0.062013, T: 1275, Avg. loss: 1904.166661\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.48, NNZs: 360, Bias: 0.075955, T: 2550, Avg. loss: 1870.009232\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.11, NNZs: 360, Bias: 0.082996, T: 3825, Avg. loss: 1846.297244\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.52, NNZs: 360, Bias: 0.072561, T: 5100, Avg. loss: 1827.322536\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.85, NNZs: 360, Bias: 0.084784, T: 6375, Avg. loss: 1811.793213\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 14.29, NNZs: 368, Bias: 0.047991, T: 1275, Avg. loss: 1461.809868\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.22, NNZs: 368, Bias: 0.070224, T: 2550, Avg. loss: 1434.688604\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.50, NNZs: 368, Bias: 0.060445, T: 3825, Avg. loss: 1415.178004\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 32.98, NNZs: 368, Bias: 0.072263, T: 5100, Avg. loss: 1400.407006\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 37.63, NNZs: 368, Bias: 0.075022, T: 6375, Avg. loss: 1387.717907\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.72, NNZs: 364, Bias: 0.065065, T: 1275, Avg. loss: 1894.559726\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.70, NNZs: 364, Bias: 0.076627, T: 2550, Avg. loss: 1859.558933\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 32.37, NNZs: 364, Bias: 0.080698, T: 3825, Avg. loss: 1833.642383\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 38.27, NNZs: 364, Bias: 0.091950, T: 5100, Avg. loss: 1813.791830\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 43.38, NNZs: 364, Bias: 0.093859, T: 6375, Avg. loss: 1797.183637\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.51, NNZs: 366, Bias: 0.071499, T: 1275, Avg. loss: 1581.276482\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.82, NNZs: 366, Bias: 0.077863, T: 2550, Avg. loss: 1550.390631\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.67, NNZs: 366, Bias: 0.093020, T: 3825, Avg. loss: 1527.185737\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 37.08, NNZs: 366, Bias: 0.088719, T: 5100, Avg. loss: 1508.783398\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 42.11, NNZs: 366, Bias: 0.092710, T: 6375, Avg. loss: 1493.309319\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.05, NNZs: 360, Bias: 0.062013, T: 1275, Avg. loss: 1904.166630\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.48, NNZs: 360, Bias: 0.075954, T: 2550, Avg. loss: 1870.008615\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.11, NNZs: 360, Bias: 0.082993, T: 3825, Avg. loss: 1846.296000\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.52, NNZs: 360, Bias: 0.072558, T: 5100, Avg. loss: 1827.320722\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.85, NNZs: 360, Bias: 0.084780, T: 6375, Avg. loss: 1811.790764\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 14.29, NNZs: 368, Bias: 0.047991, T: 1275, Avg. loss: 1461.809867\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 22.22, NNZs: 368, Bias: 0.070224, T: 2550, Avg. loss: 1434.688550\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.50, NNZs: 368, Bias: 0.060445, T: 3825, Avg. loss: 1415.177905\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 32.98, NNZs: 368, Bias: 0.072262, T: 5100, Avg. loss: 1400.406849\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 37.63, NNZs: 368, Bias: 0.075022, T: 6375, Avg. loss: 1387.717696\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.72, NNZs: 364, Bias: 0.065065, T: 1275, Avg. loss: 1894.559712\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 25.70, NNZs: 364, Bias: 0.076627, T: 2550, Avg. loss: 1859.558864\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 32.37, NNZs: 364, Bias: 0.080698, T: 3825, Avg. loss: 1833.642251\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 38.27, NNZs: 364, Bias: 0.091949, T: 5100, Avg. loss: 1813.791630\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 43.38, NNZs: 364, Bias: 0.093858, T: 6375, Avg. loss: 1797.183375\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.51, NNZs: 366, Bias: 0.071499, T: 1275, Avg. loss: 1581.276462\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.82, NNZs: 366, Bias: 0.077863, T: 2550, Avg. loss: 1550.390563\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.67, NNZs: 366, Bias: 0.093019, T: 3825, Avg. loss: 1527.185605\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 37.08, NNZs: 366, Bias: 0.088718, T: 5100, Avg. loss: 1508.783205\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 42.11, NNZs: 366, Bias: 0.092709, T: 6375, Avg. loss: 1493.309068\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 17.05, NNZs: 360, Bias: 0.062012, T: 1275, Avg. loss: 1904.166627\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 24.48, NNZs: 360, Bias: 0.075954, T: 2550, Avg. loss: 1870.008553\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 31.11, NNZs: 360, Bias: 0.082993, T: 3825, Avg. loss: 1846.295875\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.52, NNZs: 360, Bias: 0.072557, T: 5100, Avg. loss: 1827.320541\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 41.85, NNZs: 360, Bias: 0.084779, T: 6375, Avg. loss: 1811.790519\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 16.08, NNZs: 370, Bias: 0.089651, T: 1700, Avg. loss: 1707.317748\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 20.56, NNZs: 370, Bias: 0.096560, T: 3400, Avg. loss: 1684.241736\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 24.02, NNZs: 370, Bias: 0.110126, T: 5100, Avg. loss: 1668.916736\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 26.55, NNZs: 370, Bias: 0.134448, T: 6800, Avg. loss: 1657.940820\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.24, NNZs: 370, Bias: 0.147611, T: 8500, Avg. loss: 1649.060553\n",
      "Total training time: 0.00 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
       "       random_state=42, shuffle=True, verbose=1, warm_start=False),\n",
       "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
       "       param_grid={'alpha': array([  1.00000e-01,   1.00000e-02,   1.00000e-03,   1.00000e-04,\n",
       "         1.00000e-05,   1.00000e-06,   1.00000e-07])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
       "       scoring='mean_absolute_error', verbose=0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model, this might take some time...\n",
    "SGD_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing statistics:\n",
      "RMSE:34.0828162513\n",
      "MAS:23.4329222247\n"
     ]
    }
   ],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(((y - y_pred)**2).mean())\n",
    "\n",
    "print 'computing statistics:'\n",
    "y_pred = SGD_model.predict(X_test)\n",
    "print 'RMSE:' + str(rmse(y_test, y_pred))\n",
    "print 'MAS:' + str(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
