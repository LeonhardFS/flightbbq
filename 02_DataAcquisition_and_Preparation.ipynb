{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aquisition and Preparation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our second process notebook. It describes our data aquisition process (sources and characteristics), the data preparation, and finally the creation of two specific subsets for special tasks of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1 Data Aquisition Process](#1-Data-Aquisition-Process)\n",
    "    * [1.1 Flight Delay Central Database](#1.1-Flight-Delay-Central-Database)\n",
    "    * [1.2 Aircraft Information Data](#1.2-Aircraft-Information-Data)\n",
    "    * [1.3 Airport Information](#1.3-Airport-Information)\n",
    "    * [1.4 Weather Data](#1.4-Weather-Data)\n",
    "* [2 Data Preparation Process](#2-Data-Preparation-Process)\n",
    "    * [2.1 Get the Main Delay Data for 2014 from Downloaded zip Files](#2.1-Get-the-Main-Delay-Data-for-2014-from-Downloaded-zip-Files)\n",
    "    * [2.2 Combine Data with External Aircraft Data](#2.2-Combine-Data-with-External-Aircraft-Data)\n",
    "    * [2.3 Combine Data with External Airport Location Data](#2.3-Combine-Data-with-External-Airport-Location-Data)\n",
    "    * [2.4 Save the Main Final Dataset](#2.4-Save-the-Main-Final-Dataset)\n",
    "* [3 Creation of Data Subsets for Weather Analysis and Predictive Models](#3 Creation of Data Subsets for Weather Analysis and Predictive Models)\n",
    "    * [3.1 Create a Subset with External Weather Data for Selected Airports](#3.1-Create-a-Subset-with-External-Weather-Data-for-Selected-Airports)\n",
    "    * [3.2 Creation of the Prediction Datasets](#3.2-Creation-of-the-Prediction-Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Aquisition Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections will outline our data aquisition process. Furthermore, we will describe the most important characteristics and features of the datasets.\n",
    "<img src=\"images/DataSources.png\" align=\"left\" width=\"500\" height=\"500\">\n",
    "    \n",
    "   <dt>We used four different data sources</dt> \n",
    "1. The official flight database for every domestic flight in the US \n",
    "\n",
    "2. Historical weather data\n",
    "3. Airport information with geodata and names (e.g. for visualization and interpretation of results) \n",
    "4. Information about aircraft models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Flight Delay Central Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our main source of data was the Bureau of Transportation Statistics (BTS), which is as statistical agency of the US DEpartment of Transportation (<http://www.transtats.bts.gov/>). Luckily, the BTS publishes detailed data for every domestic flight in the US (<http://www.transtats.bts.gov/Tables.asp?DB_ID=120&DB_Name=Airline%20On-Time%20Performance%20Data&DB_Short_Name=On-Time>). However, it is not possible to download the data over a specified period of time, i.e. one can only download data for a month in a given year. To get the data automatically we developed a scraper tool in Python, which automatically performs the requests and downloads the data. Files adressing this issue can be found in the `src` folder. Scraping the data for ~25 years needs around 2-3 hours as requests are processed slowly on the server side. Furthermore, the BTS provides LookUp Tables for airline codes which have been downloaded manually.\n",
    "\n",
    "Since the uncompressed data for each month is around 250-300MB (comma separated), we needed to filter this dataset. A first step to do so is restricting the features. A description of all available columns is available at <http://www.transtats.bts.gov/TableInfo.asp?Table_ID=236&DB_Short_Name=On-Time&Info_Only=0>. In our analysis we will use a subset of 30 features that have been identified as relevant for the purpose of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Aircraft Information Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of us have experienced it before: a flight is delayed because there are some lastminute repairs or other problems with the aircraft. We are curious if the manufacturer or the age of the aircraft influences the probability of delays. Therefore we need more detailed data about the flight. From the delay dataset mentioned in 2.1 we get the tail number of the aircraft for every single flight. This tail number is comparable to car license plates and helps us to identify the manufacturer and age of the airplane. \n",
    "\n",
    "We can get this information from the Federal Aviation Administration. This institution has a database of tail numbers (so called N-Numbers) for each aircraft in the US and also publishes other datasets with information about the respective aircraft (e.g. manufacturer of turbines, owner, etc.). We downloaded the database from the following website:\n",
    "http://www.faa.gov/licenses_certificates/aircraft_certification/aircraft_registry/releasable_aircraft_download/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Airport Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we need the exact geolocations for the airports in the dataset, for example to get good visualizations using Tableau Public or other maps. Furthermore, airport names would be helpful to interpret the data (the original dataset just contains the short IATA abbreviations). This data can be easily found online as csvs (see http://openflights.org/data.html). It can be found in the folder `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural cause for many delays seems to be the weather. We decided to include weather data additionally in our analysis. Therefore, we wanted to get historical weather information of the airports. Unfortunately, when it comes to weather data, we couldn't find any public available sources that provide a suitable (free) dataset. However, Wunderground provides a webinterface allowing to query specific IATA / IAOC codes of airports (see i.e. <http://www.wunderground.com/history/airport/EDDF/2005/10/3/DailyHistory.html?req_city=Frankfurt+%2F+Main&req_state=&req_statename=Germany&reqdb.zip=00000&reqdb.magic=5&reqdb.wmo=10637>). Writing a script allowed us to get historic data of individual airports. \n",
    "\n",
    "<table>\n",
    "<tr><td>**events**</td><td>a list containing strings of weather events, i.e. \"Rain\", \"Fog\", \"Snow\"</td></tr>\n",
    "<tr><td>**humidity**</td><td>humidity measured in percent</td></tr>\n",
    "<tr><td>**precipitation**</td><td>precipitation measured in inches </td></tr>\n",
    "<tr><td>**sealevelpressure**</td><td>pressure at sea level in inches</td></tr>\n",
    "<tr><td>**snowdepth**</td><td>snow depth in inches</td></tr>\n",
    "<tr><td>**snowfall**</td><td>snow fall in inches</td></tr>\n",
    "<tr><td>**temperature**</td><td>temperature in degree Fahrenheit</td></tr>\n",
    "<tr><td>**visibility**</td><td>visibility in miles</td></tr>\n",
    "<tr><td>**windspeed**</td><td>wind speed in miles per hour</td></tr>\n",
    "</table>\n",
    " \n",
    "One drawback of this method is similiar to getting the data from the BTS: the slow processing of requests from the server and the amount of requests necessary to get data matching the huge dataset of the BTS (ca. 15 min for a single year and just one airport). Thus, we decided to focus on the weather at the John F. Kennedy International Airport (New York City) and at the Boston Logan International Airport (Boston) only. Although we just used these two airports we could get some very valuable insights about weather's effects on delays (see process notebook for exploratory analysis). The detailed code for the webscraper can be found in the `src`-folder. It has not been included in this notebook, as it is not part of our actual analysis and would affect the readability of this notebook. The scraper creates a file `weather_data.json` in the `data`-folder that can be used for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Preparation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required modules for data preparation tasks\n",
    "import requests, zipfile, StringIO\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import re\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get the Main Delay Data for 2014 from Downloaded zip Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to open and combine the zipped data files for each month of the delay data that has been downloaded according to the process outlined in the data aquisition section above. As we have more than 400,000 recorded flights each month, the dataset is extremely large. We therefore decided to focus on the subset of all flights in 2014 to get the most relevant flight information without missing certain months (this could be important when investigating seasonality effects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 01\n",
      "Downloaded 02\n",
      "Downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:1170: DtypeWarning: Columns (69,74) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n",
      "/Users/David/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:1170: DtypeWarning: Columns (63,68,69,74) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 03\n",
      "Downloaded 04\n",
      "Downloaded 05\n",
      "Downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:1170: DtypeWarning: Columns (42,69,74) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n",
      "/Users/David/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:1170: DtypeWarning: Columns (68,69,74) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 06\n",
      "Downloaded 07\n",
      "Downloaded 08\n",
      "Downloaded 09\n",
      "Downloaded 10\n",
      "Downloaded 11\n",
      "Downloaded 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/lib/python2.7/site-packages/pandas/io/parsers.py:1170: DtypeWarning: Columns (42,63,68,69,74) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    }
   ],
   "source": [
    "# reads all predefined months for a year and merge into one data frame\n",
    "rawData = pd.DataFrame()\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "for m in months:\n",
    "    z = zipfile.ZipFile('cache/{y}{mo}.zip'.format(y=str(2014), mo = m))\n",
    "    rawData = rawData.append(pd.read_csv(z.open(z.namelist()[0])))\n",
    "    print \"Downloaded\", m\n",
    "# reset index of complete dataset for delays to prepare merging in next step\n",
    "rawData.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns we now have in the dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'index', u'YEAR', u'QUARTER', u'MONTH', u'DAY_OF_MONTH',\n",
       "       u'DAY_OF_WEEK', u'FL_DATE', u'UNIQUE_CARRIER', u'AIRLINE_ID',\n",
       "       u'CARRIER', u'TAIL_NUM', u'FL_NUM', u'ORIGIN', u'ORIGIN_CITY_NAME',\n",
       "       u'ORIGIN_STATE_ABR', u'ORIGIN_STATE_FIPS', u'ORIGIN_STATE_NM',\n",
       "       u'ORIGIN_WAC', u'DEST', u'DEST_CITY_NAME', u'DEST_STATE_ABR',\n",
       "       u'DEST_STATE_FIPS', u'DEST_STATE_NM', u'DEST_WAC', u'CRS_DEP_TIME',\n",
       "       u'DEP_TIME', u'DEP_DELAY', u'DEP_DELAY_NEW', u'DEP_DEL15',\n",
       "       u'DEP_DELAY_GROUP', u'DEP_TIME_BLK', u'TAXI_OUT', u'WHEELS_OFF',\n",
       "       u'WHEELS_ON', u'TAXI_IN', u'CRS_ARR_TIME', u'ARR_TIME', u'ARR_DELAY',\n",
       "       u'ARR_DELAY_NEW', u'ARR_DEL15', u'ARR_DELAY_GROUP', u'ARR_TIME_BLK',\n",
       "       u'CANCELLED', u'CANCELLATION_CODE', u'DIVERTED', u'CRS_ELAPSED_TIME',\n",
       "       u'ACTUAL_ELAPSED_TIME', u'AIR_TIME', u'FLIGHTS', u'DISTANCE',\n",
       "       u'DISTANCE_GROUP', u'CARRIER_DELAY', u'WEATHER_DELAY', u'NAS_DELAY',\n",
       "       u'SECURITY_DELAY', u'LATE_AIRCRAFT_DELAY', u'FIRST_DEP_TIME',\n",
       "       u'TOTAL_ADD_GTIME', u'LONGEST_ADD_GTIME', u'DIV_AIRPORT_LANDINGS',\n",
       "       u'DIV_REACHED_DEST', u'DIV_ACTUAL_ELAPSED_TIME', u'DIV_ARR_DELAY',\n",
       "       u'DIV_DISTANCE', u'DIV1_AIRPORT', u'DIV1_WHEELS_ON',\n",
       "       u'DIV1_TOTAL_GTIME', u'DIV1_LONGEST_GTIME', u'DIV1_WHEELS_OFF',\n",
       "       u'DIV1_TAIL_NUM', u'DIV2_AIRPORT', u'DIV2_WHEELS_ON',\n",
       "       u'DIV2_TOTAL_GTIME', u'DIV2_LONGEST_GTIME', u'DIV2_WHEELS_OFF',\n",
       "       u'DIV2_TAIL_NUM', u'DIV3_AIRPORT', u'DIV3_WHEELS_ON',\n",
       "       u'DIV3_TOTAL_GTIME', u'DIV3_LONGEST_GTIME', u'DIV3_WHEELS_OFF',\n",
       "       u'DIV3_TAIL_NUM', u'DIV4_AIRPORT', u'DIV4_WHEELS_ON',\n",
       "       u'DIV4_TOTAL_GTIME', u'DIV4_LONGEST_GTIME', u'DIV4_WHEELS_OFF',\n",
       "       u'DIV4_TAIL_NUM', u'DIV5_AIRPORT', u'DIV5_WHEELS_ON',\n",
       "       u'DIV5_TOTAL_GTIME', u'DIV5_LONGEST_GTIME', u'DIV5_WHEELS_OFF',\n",
       "       u'DIV5_TAIL_NUM', u'Unnamed: 93'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we just need a subset of these columns for our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selectedColumns = [u'index', u'FL_DATE', u'UNIQUE_CARRIER', u'TAIL_NUM', u'FL_NUM', \n",
    "                   u'ORIGIN', u'DEST', u'CRS_DEP_TIME', u'DEP_TIME', u'DEP_DELAY', u'TAXI_OUT', \n",
    "                   u'WHEELS_OFF', u'WHEELS_ON', u'TAXI_IN', u'CRS_ARR_TIME', u'ARR_TIME', u'ARR_DELAY', \n",
    "                   u'CANCELLED', u'DIVERTED', u'CANCELLATION_CODE', u'AIR_TIME', u'DISTANCE', \n",
    "                   u'CARRIER_DELAY', u'WEATHER_DELAY', u'NAS_DELAY', u'SECURITY_DELAY', u'LATE_AIRCRAFT_DELAY',\n",
    "                   u'ORIGIN_CITY_NAME', u'DEST_CITY_NAME']\n",
    "rawData = rawData[selectedColumns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Combine Data with External Aircraft Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have two tables containing infos about the aircraft and its manufacturer available as comma separated textfiles in the `data`-folder as outlined in the section above. Both files will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = zipfile.ZipFile('externalData/AircraftInformation.zip')\n",
    "# master table containing tail numbers of aircraft\n",
    "df_master  = pd.DataFrame.from_csv(z.open('MASTER.txt'))\n",
    "# detailed table containing information about manufacturer, age, etc.\n",
    "df_aircrafts  = pd.DataFrame.from_csv(z.open('ACFTREF.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now join these two tables based on their common ID that is saved in the column `MFR MDL CODE` of the master table and in the index of the aircraft table respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master = df_master[['MFR MDL CODE', 'YEAR MFR']].reset_index()\n",
    "aircrafts = df_aircrafts['MFR'].reset_index()\n",
    "master.columns = ['TAIL_NUM', 'CODE', 'YEAR']\n",
    "aircrafts.columns = ['CODE', 'MFR']\n",
    "joined = pd.merge(master, aircrafts, how='left', on='CODE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join this aircraft information with our delay data and extend the original dataset with the two new features: The year in which the aircraft was built (to determine the age) and the manufacturer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda/lib/python2.7/site-packages/pandas/core/generic.py:2177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "delayFinal = rawData[['TAIL_NUM','UNIQUE_CARRIER']]\n",
    "delayFinal.TAIL_NUM = delayFinal.TAIL_NUM.str.strip('N')\n",
    "delaymfr = pd.merge(delayFinal, joined, how='left', on=['TAIL_NUM'])\n",
    "rawData['AIRCRAFT_YEAR'] = delaymfr.YEAR\n",
    "rawData['AIRCRAFT_MFR'] = delaymfr.MFR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Combine Data with External Airport Location Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load an external dataset that contains the geolocations for each commercial airport in the world. We filter this to get only the airports in the US and then assign the respective geocode of the origin airport to our original delay dataset by merging both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airportLocation = pd.DataFrame.from_csv('externalData/airport_codes_with_geo_name_ids_and_nl_names-2008-04-14.csv', header=None)\n",
    "usAirports = airportLocation[airportLocation[4]=='US'].reset_index()\n",
    "# we just need a subsets of the columns (origin, latitude and longitude)\n",
    "usAirports = usAirports[[0, 5, 6]]\n",
    "usAirports.columns = ['ORIGIN', 'LAT', 'LONG']\n",
    "complete2014Data = pd.merge(rawData, usAirports, how='left', on='ORIGIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007050916258277116"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0*np.sum(complete2014Data.LAT.isnull())/complete2014Data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just 0.7% of alll flight origins could not be located, so the merge was quite successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save the Main Final Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataframe `complete2014Data` will be locally stored as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "complete2014Data.to_csv('cache/complete2014Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Creation of Data Subsets for Weather Analysis and Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create a Subset with External Weather Data for Selected Airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As outlined in previous section, we also scraped historical weather data for major US airports from the web. This data can be used as additional features for each flight to get information about the current weather conditions at the airport of departure. The script assumes that there is the `weather_data.json` file in the `data`-folder and that this file contains the respective weather information for the JFK airport in new york and the BOS airport in Boston for each day in 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the weather file\n",
    "weatherFile = os.path.join('data', 'weather_data.json')\n",
    "with open(weatherFile) as infile:\n",
    "    weatherDict = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract the weather data for new york and boston out of the json file and save it in weather_df\n",
    "dates = []\n",
    "frames = []\n",
    "\n",
    "# create df for weather in new york\n",
    "for datapoint in weatherDict['JFK']:\n",
    "    date = datapoint['date']\n",
    "    frames.append(pd.DataFrame(datapoint['data'], index=['%s-%s-%s' % (date[0:4], date[4:6], date[6:8])]))\n",
    "weather_jfk = pd.concat(frames).reset_index()\n",
    "\n",
    "# create df for weather in boston\n",
    "for datapoint in weatherDict['BOS']:\n",
    "    date = datapoint['date']\n",
    "    frames.append(pd.DataFrame(datapoint['data'], index=['%s-%s-%s' % (date[0:4], date[4:6], date[6:8])]))\n",
    "weather_bos = pd.concat(frames).reset_index()\n",
    "\n",
    "# get just the departures for the John F. Kennedy airport in New York City and Logan airport in Boston\n",
    "jfk_delays = complete2014Data[complete2014Data.ORIGIN=='JFK']\n",
    "bos_delays = complete2014Data[complete2014Data.ORIGIN=='BOS']\n",
    "\n",
    "# merge delays with weather_df created above\n",
    "jfk_dalayWeather = pd.merge(jfk_delays, weather_jfk, how='left', left_on='FL_DATE', right_on = 'index')\n",
    "bos_dalayWeather = pd.merge(bos_delays, weather_bos, how='left', left_on='FL_DATE', right_on = 'index')\n",
    "\n",
    "jfk_bos_comparison = pd.concat([jfk_dalayWeather, bos_dalayWeather]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save everything in a csv\n",
    "jfk_bos_comparison.to_csv('cache/jfk_bos_weather_2014.csv', encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.2 Creation of the Prediction Datasets\n",
    "Before evaluating any models on the data, we have to clean it a bit.\n",
    "\n",
    "#### Pruning the data\n",
    "When cleaning the data set, we have to remove the following entries:\n",
    "\n",
    "- flights that have been cancelled or diverted. We focus on predicting the delay. As a result, we also remove the columns associated with diverted flights.\n",
    "- colmuns that give the answer. This is the case of many colmuns related to the arrival of the plane\n",
    "- rows where a value is missing\n",
    "\n",
    "Note that data points have to be cleaned in this order because most flights have empty entries for the 'diverted' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#entries to be dropped in the analysis\n",
    "flight_data_dropped = ['QUARTER', 'DAY_OF_MONTH', 'AIRLINE_ID', 'CARRIER', 'FL_NUM', 'TAIL_NUM']\n",
    "\n",
    "location_data_dropped = ['ORIGIN_STATE_FIPS', 'ORIGIN_STATE_NM',\\\n",
    "                    'ORIGIN_WAC', 'DEST_STATE_FIPS', \\\n",
    "                    'DEST_STATE_NM', 'DEST_WAC']\n",
    "\n",
    "departure_data_dropped = ['DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'DEP_DELAY_GROUP',\\\n",
    "                          'DEP_TIME_BLK', 'TAXI_OUT', 'WHEELS_OFF']\n",
    "\n",
    "arrival_data_dropped = ['WHEELS_ON', 'TAXI_IN', 'ARR_TIME', 'ARR_DELAY_NEW',\\\n",
    "                       'ARR_DELAY_GROUP', 'ARR_TIME_BLK']\n",
    "\n",
    "cancel_data_dropped = ['CANCELLED','CANCELLATION_CODE', 'DIVERTED']\n",
    "\n",
    "summaries_dropped = ['CRS_ELAPSED_TIME', 'AIR_TIME', 'FLIGHTS']\n",
    "\n",
    "cause_delay_dropped = ['CARRIER_DELAY', 'WEATHER_DELAY', 'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY']\n",
    "\n",
    "gate_return_dropped = ['FIRST_DEP_TIME', 'TOTAL_ADD_GTIME', 'LONGEST_ADD_GTIME']\n",
    "\n",
    "diverted_data_dropped = ['DIV_AIRPORT_LANDINGS', 'DIV_REACHED_DEST', 'DIV_ACTUAL_ELAPSED_TIME',  \\\n",
    "                         'DIV_ARR_DELAY', 'DIV_DISTANCE', 'DIV1_AIRPORT', 'DIV1_WHEELS_ON', \\\n",
    "                         'DIV1_TOTAL_GTIME', 'DIV1_LONGEST_GTIME', 'DIV1_WHEELS_OFF', \\\n",
    "                         'DIV1_TAIL_NUM', 'DIV2_AIRPORT', 'DIV2_WHEELS_ON', \\\n",
    "                         'DIV2_TOTAL_GTIME', 'DIV2_LONGEST_GTIME', 'DIV2_WHEELS_OFF', \\\n",
    "                         'DIV2_TAIL_NUM', 'DIV3_AIRPORT', 'DIV3_WHEELS_ON', \\\n",
    "                         'DIV3_TOTAL_GTIME', 'DIV3_LONGEST_GTIME', 'DIV3_WHEELS_OFF', 'DIV3_TAIL_NUM', \\\n",
    "                         'DIV4_AIRPORT', 'DIV4_WHEELS_ON', 'DIV4_TOTAL_GTIME', 'DIV4_LONGEST_GTIME', \\\n",
    "                         'DIV4_WHEELS_OFF', 'DIV4_TAIL_NUM', 'DIV5_AIRPORT', 'DIV5_WHEELS_ON', \\\n",
    "                         'DIV5_TOTAL_GTIME', 'DIV5_LONGEST_GTIME', 'DIV5_WHEELS_OFF', 'DIV5_TAIL_NUM']\n",
    "\n",
    "other_dropped = ['Unnamed: 93']\n",
    "\n",
    "columns_dropped = flight_data_dropped + location_data_dropped + departure_data_dropped + arrival_data_dropped \\\n",
    "    + cancel_data_dropped + summaries_dropped + cause_delay_dropped + gate_return_dropped + diverted_data_dropped \\\n",
    "    + other_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(data, list_col):\n",
    "    ''' \n",
    "    Creates a dataset by excluding undesirable columns\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    df: pandas.DataFrame\n",
    "       Flight dataframe  \n",
    "\n",
    "    list_col: <list 'string'>\n",
    "        Comumns to exclude from the data set\n",
    "    '''\n",
    "\n",
    "    # security check to drop only columns that exist\n",
    "    list_col = list(set(list_col) & set(data.columns))\n",
    "    \n",
    "    res = data[(data.CANCELLED == 0) & (data.DIVERTED == 0)]\n",
    "    res.drop(list_col, axis=1, inplace=True)\n",
    "    res.dropna(axis = 0, inplace = True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'index', u'FL_DATE', u'UNIQUE_CARRIER', u'ORIGIN', u'DEST',\n",
      "       u'CRS_DEP_TIME', u'CRS_ARR_TIME', u'ARR_DELAY', u'DISTANCE',\n",
      "       u'ORIGIN_CITY_NAME', u'DEST_CITY_NAME', u'AIRCRAFT_YEAR',\n",
      "       u'AIRCRAFT_MFR', u'LAT', u'LONG'],\n",
      "      dtype='object')\n",
      "CPU times: user 4.48 s, sys: 1.53 s, total: 6.01 s\n",
      "Wall time: 6.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data2014 = clean(complete2014Data, columns_dropped)\n",
    "print data2014.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the data for active Airlines only\n",
    "As we want to predict delay times, we throw out any flights that are operated by a shutdown airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIQUE_CARRIER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  UNIQUE_CARRIER\n",
       "0             M3\n",
       "1             JP\n",
       "2             A3\n",
       "3             EI\n",
       "4             NG"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_active_airlines = pd.read_csv('data/cur_airlines.txt', header=None)\n",
    "df_active_airlines.columns = [['UNIQUE_CARRIER']];\n",
    "df_active_airlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filteredData2014 = data2014.merge(df_active_airlines, on=['UNIQUE_CARRIER', 'UNIQUE_CARRIER'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check reveals, that filtering was not (really) necessary as all Airlines are still active today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4103597, 4103597)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filteredData2014.count()[0], data2014.count()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save data to cache\n",
    "filteredData2014.to_csv('cache/linear_model_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
